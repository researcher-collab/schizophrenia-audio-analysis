
# -------------------------
# Imports
# -------------------------
import os

import datetime
import warnings

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras import layers, models

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    accuracy_score,
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold

import joblib


# ==========================================================
# Utility: class distribution
# ==========================================================
def class_distribution(y_labels):
    """
    Compute the class distribution as a dictionary: {class: count}.

    Parameters
    ----------
    y_labels : array-like
        1D array of labels (0/1 or similar).

    Returns
    -------
    dict
        Mapping from label value to count.
    """
    values, counts = np.unique(y_labels, return_counts=True)
    return dict(zip(values, counts))


# ==========================================================
# CONFIGURATION
# ==========================================================

# Path to feature CSV generated by the feature extraction script
DATA_CSV_PATH = "datafile.csv"

# dtype for File column (keep as string)
dtype_dict = {'File': 'object'}

# Global list to record evaluation results for final summary CSV
results_records = []

# Initialize warnings
warnings.filterwarnings('ignore')

# Create results directory with timestamp
now = datetime.datetime.now()
results_dir_name = now.strftime("resultsCondensed%m%d%Y%H%M")
os.makedirs(results_dir_name, exist_ok=True)

# Directory for saving trained models
models_dir = os.path.join(results_dir_name, "models")
os.makedirs(models_dir, exist_ok=True)


# ==========================================================
# Load Data
# ==========================================================
# The CSV is expected to contain:
#   - 'File' column (relative or absolute path to audio file)
#   - Acoustic / embedding feature columns
#   - Possibly stale metadata columns which will be replaced.
data = pd.read_csv(DATA_CSV_PATH, dtype=dtype_dict)


# ==========================================================
# Metadata Extraction from File Path
# ==========================================================
def extract_metadata(file_path: str) -> pd.Series:
    """
    Derive metadata from the audio filename / path.

    Heuristics used:
      - ID: first token before the first underscore if numeric.
      - Race: infer from tokens in path (b/black, w/white, a/asian,
              h/hispanic, o/other).
      - Gender: infer from tokens in path (m/male, f/female).
      - Role:  'nurse', 'patient', or 'unknown' (speaker0/1 treated as unknown).
      - Group: 'schizophrenia' if 'pos' appears in path,
               'control' if 'neg' or 'control' appears.

    Parameters
    ----------
    file_path : str
        File path as stored in the 'File' column.

    Returns
    -------
    pd.Series
        With keys: ID, Group, Gender, Race, Role
    """
    lower_path = file_path.lower()
    fname = os.path.basename(file_path)
    stem, _ = os.path.splitext(fname)

    # ID: numeric prefix before first underscore
    id_str = stem.split('_')[0]
    id_str = id_str if id_str.isdigit() else None

    # Race detection
    race = 'unknown'
    if any(k in lower_path for k in ['_b_', '_b', 'black']):
        race = 'black'
    elif any(k in lower_path for k in ['_w_', '_w', 'white']):
        race = 'white'
    elif any(k in lower_path for k in ['_a_', '_a', 'asian']):
        race = 'asian'
    elif any(k in lower_path for k in ['_h_', '_h', 'hispanic']):
        race = 'hispanic'
    elif any(k in lower_path for k in ['_o_', '_o', 'other']):
        race = 'other'

    # Gender detection
    gender = 'unknown'
    if any(k in lower_path for k in ['_m_', '_m', 'male']):
        gender = 'male'
    elif any(k in lower_path for k in ['_f_', '_f', 'female']):
        gender = 'female'

    # Role detection
    if 'nurse' in lower_path:
        role = 'nurse'
    elif 'patient' in lower_path:
        role = 'patient'
    elif 'speaker0' in lower_path or 'speaker1' in lower_path:
        role = 'unknown'
    else:
        role = 'unknown'

    # Group detection
    if 'pos' in lower_path:
        group = 'schizophrenia'
    elif 'neg' in lower_path or 'control' in lower_path:
        group = 'control'
    else:
        group = 'unknown'

    return pd.Series({
        'ID': id_str,
        'Group': group,
        'Gender': gender,
        'Race': race,
        'Role': role
    })


# ==========================================================
# Prepare Dataset
# ==========================================================
metadata_columns = ['File', 'ID', 'Group', 'Gender', 'Race', 'Role']

# 1) Drop any existing metadata (except File) to avoid stale values
data = data.drop(
    columns=[c for c in metadata_columns if c != 'File' and c in data.columns],
    errors='ignore'
)

# 2) Add fresh metadata derived from 'File'
data = data.join(data['File'].apply(extract_metadata))
data['ID'] = pd.to_numeric(data['ID'], errors='coerce').astype('Int64')

# 3) Filter
data = data[data['Role'] = 'patient']

# 4) Keep only rows with fully labelled Group/Gender/Race/ID
data = data[
    (data['Group']  != 'unknown') &
    (data['Gender'] != 'unknown') &
    (data['Race']   != 'unknown') &
    (data['ID'].notna())
]

print(f"\nInitial dataset size after role filtering: {len(data)} samples\n")
print("Group counts:")
print(data['Group'].value_counts())
print("\nGender/Race/Group breakdown:")
print(
    data[['Gender', 'Race', 'Group']]
      .groupby(['Gender', 'Race', 'Group'])
      .size()
      .unstack(fill_value=0)
)

# ----------------------------------------------------------
# Feature cleaning
# ----------------------------------------------------------
original_feature_columns = [c for c in data.columns if c not in metadata_columns]

# 4-a) Replace Â±inf with NaN and coerce all to numeric
features_df = (
    data[original_feature_columns]
        .replace([np.inf, -np.inf], np.nan)
        .apply(pd.to_numeric, errors='coerce')
)

# 4-b) Fill NaNs with per-column mean (simple imputation)
features_df = features_df.fillna(features_df.mean(numeric_only=True))

# Put cleaned features back into data frame
data[original_feature_columns] = features_df

# Remove zero-variance features (uninformative)
variance_selector = VarianceThreshold()
variance_selector.fit(data[original_feature_columns])
features_to_keep = data[original_feature_columns].columns[
    variance_selector.get_support()
]

# Final dataset: keep only cleaned features + labels
data = data[features_to_keep.tolist() + ['Group', 'Gender', 'Race']]

# ----------------------------------------------------------
# Class balancing
# ----------------------------------------------------------
min_class_size = data['Group'].value_counts().min()
balanced_data = (
    data.groupby('Group', group_keys=False)
        .apply(lambda x: x.sample(min_class_size, random_state=42))
        .reset_index(drop=True)
)
data = balanced_data

print(f"\nDataset balanced to {min_class_size} samples per class.")
print(data['Group'].value_counts())

# Feature matrix and binary label vector
X = data[features_to_keep]
y = data['Group'].map({'control': 0, 'schizophrenia': 1}).astype('int8')


# ==========================================================
# Normalize Features
# ==========================================================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# ==========================================================
# Feature Selection (Univariate ANOVA F-test)
# ==========================================================
k_features = min(50, X_scaled.shape[1])  
kbest_selector = SelectKBest(score_func=f_classif, k=k_features)
X_selected = kbest_selector.fit_transform(X_scaled, y)
selected_features = features_to_keep[kbest_selector.get_support(indices=True)]

print(f"\nSelected top {k_features} features:")
print(selected_features)


# ==========================================================
# Model Evaluation Helpers (Sklearn)
# ==========================================================
def evaluate_model(
    model,
    model_name,
    X_train,
    X_test,
    y_train,
    y_test,
    group_name: str = "Full_Group",
):
    """
    Fit a scikit-learn classifier, compute predictions and metrics,
    and save confusion matrix + ROC curve plots.

    Also records a row in `results_records`.
    """
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Probabilities for ROC; if not available, fall back to decision_function
    if hasattr(model, 'predict_proba'):
        y_prob = model.predict_proba(X_test)[:, 1]
    elif hasattr(model, 'decision_function'):
        y_scores = model.decision_function(X_test)
        # Normalize to [0, 1] for ROC/AUC
        y_prob = (y_scores - y_scores.min()) / (y_scores.max() - y_scores.min())
    else:
        y_prob = y_pred

    accuracy = accuracy_score(y_test, y_pred)

    print(f"\nClassification Report for {model_name} ({group_name}):")
    print(classification_report(y_test, y_pred, target_names=['Control', 'Schizophrenia']))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Control', 'Schizophrenia'],
        yticklabels=['Control', 'Schizophrenia'],
    )
    plt.title(f'Confusion Matrix for {model_name} ({group_name})\nAccuracy: {accuracy:.2f}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir_name, f'confusion_matrix_{model_name}_{group_name}.png'))
    plt.close()

    # ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    auc = roc_auc_score(y_test, y_prob)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.title(f'ROC Curve for {model_name} ({group_name})\nAccuracy: {accuracy:.2f}')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir_name, f'roc_curve_{model_name}_{group_name}.png'))
    plt.close()

    # Detailed metrics
    report_dict = classification_report(
        y_test,
        y_pred,
        target_names=['Control', 'Schizophrenia'],
        output_dict=True,
    )
    train_dist = class_distribution(y_train)
    test_dist = class_distribution(y_test)

    results_records.append({
        "Model": model_name,
        "Group": group_name,
        "Train_Control_Count": train_dist.get(0, 0),
        "Train_Schizophrenia_Count": train_dist.get(1, 0),
        "Test_Control_Count": test_dist.get(0, 0),
        "Test_Schizophrenia_Count": test_dist.get(1, 0),
        "Accuracy": accuracy,
        "AUC": auc,
        "Control_Precision": report_dict["Control"]["precision"],
        "Control_Recall": report_dict["Control"]["recall"],
        "Control_F1": report_dict["Control"]["f1-score"],
        "Schizophrenia_Precision": report_dict["Schizophrenia"]["precision"],
        "Schizophrenia_Recall": report_dict["Schizophrenia"]["recall"],
        "Schizophrenia_F1": report_dict["Schizophrenia"]["f1-score"],
        "Macro_Avg_F1": report_dict["macro avg"]["f1-score"],
        "Time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    })


# ==========================================================
# Model Evaluation Helpers (Keras)
# ==========================================================
def evaluate_model_keras(
    model,
    model_name,
    X_train,
    X_test,
    y_train,
    y_test,
    group_name: str = "Full_Group",
):
    """
    Fit a Keras model (binary classification), compute predictions and metrics,
    and save confusion matrix + ROC plots.

    Also records a row in `results_records`.
    """
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

    y_prob = model.predict(X_test).flatten()
    y_pred = (y_prob > 0.5).astype("int32")

    accuracy = accuracy_score(y_test, y_pred)

    print(f"\nClassification Report for {model_name} ({group_name}):")
    print(classification_report(y_test, y_pred, target_names=['Control', 'Schizophrenia']))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Control', 'Schizophrenia'],
        yticklabels=['Control', 'Schizophrenia'],
    )
    plt.title(f'Confusion Matrix for {model_name} ({group_name})\nAccuracy: {accuracy:.2f}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir_name, f'confusion_matrix_{model_name}_{group_name}.png'))
    plt.close()

    # ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    auc = roc_auc_score(y_test, y_prob)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.title(f'ROC Curve for {model_name} ({group_name})\nAccuracy: {accuracy:.2f}')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir_name, f'roc_curve_{model_name}_{group_name}.png'))
    plt.close()

    report_dict = classification_report(
        y_test,
        y_pred,
        target_names=['Control', 'Schizophrenia'],
        output_dict=True,
    )
    train_dist = class_distribution(y_train)
    test_dist = class_distribution(y_test)

    results_records.append({
        "Model": model_name,
        "Group": group_name,
        "Train_Control_Count": train_dist.get(0, 0),
        "Train_Schizophrenia_Count": train_dist.get(1, 0),
        "Test_Control_Count": test_dist.get(0, 0),
        "Test_Schizophrenia_Count": test_dist.get(1, 0),
        "Accuracy": accuracy,
        "AUC": auc,
        "Control_Precision": report_dict["Control"]["precision"],
        "Control_Recall": report_dict["Control"]["recall"],
        "Control_F1": report_dict["Control"]["f1-score"],
        "Schizophrenia_Precision": report_dict["Schizophrenia"]["precision"],
        "Schizophrenia_Recall": report_dict["Schizophrenia"]["recall"],
        "Schizophrenia_F1": report_dict["Schizophrenia"]["f1-score"],
        "Macro_Avg_F1": report_dict["macro avg"]["f1-score"],
        "Time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    })


# ==========================================================
# Keras Model Definitions (LSTM, GRU, 1D-CNN)
# ==========================================================
def create_lstm_model(input_shape):
    """
    Simple LSTM-based binary classifier.
    """
    model = models.Sequential()
    model.add(layers.LSTM(64, input_shape=input_shape))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'],
    )
    return model


def create_gru_model(input_shape):
    """
    Simple GRU-based binary classifier.
    """
    model = models.Sequential()
    model.add(layers.GRU(64, input_shape=input_shape))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'],
    )
    return model


def create_cnn_model(input_shape):
    """
    Simple 1D CNN binary classifier for feature sequences.
    """
    model = models.Sequential()
    model.add(layers.Conv1D(32, 3, activation='relu', input_shape=input_shape))
    model.add(layers.MaxPooling1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'],
    )
    return model


# ==========================================================
# Perform Classification (Full dataset)
# ==========================================================
print("\nPerforming classification for the entire dataset (Full Group)...")
print(f"Total dataset size: {len(data)} samples")

X_train, X_test, y_train, y_test = train_test_split(
    X_selected,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y,
)
print(f"Training set size: {len(X_train)} samples")
print(f"Testing set size: {len(X_test)} samples")

# Initialize traditional ML models
rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
)
svm_model = SVC(
    kernel='rbf',
    probability=True,
    random_state=42,
)
mlp_model = MLPClassifier(
    hidden_layer_sizes=(50, 25),
    max_iter=500,
    random_state=42,
)

# Evaluate and save traditional ML models
evaluate_model(rf_model, 'Random_Forest', X_train, X_test, y_train, y_test)
joblib.dump(rf_model, os.path.join(models_dir, 'rf_model.joblib'))

evaluate_model(svm_model, 'SVM', X_train, X_test, y_train, y_test)
joblib.dump(svm_model, os.path.join(models_dir, 'svm_model.joblib'))

evaluate_model(mlp_model, 'Neural_Network', X_train, X_test, y_train, y_test)
joblib.dump(mlp_model, os.path.join(models_dir, 'mlp_model.joblib'))

# Reshape for sequence models
X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Reshape for CNN (time x channels)
X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# LSTM
lstm_model = create_lstm_model((1, X_train.shape[1]))
evaluate_model_keras(
    lstm_model,
    'LSTM',
    X_train_lstm,
    X_test_lstm,
    y_train,
    y_test,
)
lstm_model.save(os.path.join(models_dir, 'lstm_model.h5'))

# GRU
gru_model = create_gru_model((1, X_train.shape[1]))
evaluate_model_keras(
    gru_model,
    'GRU',
    X_train_lstm,
    X_test_lstm,
    y_train,
    y_test,
)
gru_model.save(os.path.join(models_dir, 'gru_model.h5'))

# CNN
cnn_model = create_cnn_model((X_train.shape[1], 1))
evaluate_model_keras(
    cnn_model,
    'CNN',
    X_train_cnn,
    X_test_cnn,
    y_train,
    y_test,
)
cnn_model.save(os.path.join(models_dir, 'cnn_model.h5'))

# Optionally save preprocessing objects
joblib.dump(scaler, os.path.join(models_dir, 'scaler.joblib'))
joblib.dump(kbest_selector, os.path.join(models_dir, 'kbest_selector.joblib'))
joblib.dump(variance_selector, os.path.join(models_dir, 'variance_selector.joblib'))


# ==========================================================
# Ensemble (Soft Voting over 6 models)
# ==========================================================
print("\nEvaluating Ensemble model (Full_Group)...")

rf_probs   = rf_model.predict_proba(X_test)[:, 1]
svm_probs  = svm_model.predict_proba(X_test)[:, 1]
mlp_probs  = mlp_model.predict_proba(X_test)[:, 1]
lstm_probs = lstm_model.predict(X_test_lstm).flatten()
gru_probs  = gru_model.predict(X_test_lstm).flatten()
cnn_probs  = cnn_model.predict(X_test_cnn).flatten()

# These weights reflect relative trust in each model.
weights = {
    "rf":   0.15,
    "svm":  0.15,
    "mlp":  0.10,
    "lstm": 0.25,
    "gru":  0.25,
    "cnn":  0.10,
}

ensemble_prob = (
    weights["rf"]   * rf_probs   +
    weights["svm"]  * svm_probs  +
    weights["mlp"]  * mlp_probs  +
    weights["lstm"] * lstm_probs +
    weights["gru"]  * gru_probs  +
    weights["cnn"]  * cnn_probs
)
ensemble_pred = (ensemble_prob > 0.5).astype("int32")

accuracy = accuracy_score(y_test, ensemble_pred)
report_dict = classification_report(
    y_test,
    ensemble_pred,
    target_names=['Control', 'Schizophrenia'],
    output_dict=True,
)
auc = roc_auc_score(y_test, ensemble_prob)
fpr, tpr, _ = roc_curve(y_test, ensemble_prob)

# Confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(
    confusion_matrix(y_test, ensemble_pred),
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Control', 'Schizophrenia'],
    yticklabels=['Control', 'Schizophrenia'],
)
plt.title(f'Confusion Matrix for Ensemble (Full_Group)\nAccuracy: {accuracy:.2f}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.savefig(os.path.join(results_dir_name, f'confusion_matrix_Ensemble_Full_Group.png'))
plt.close()

# ROC curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title(f'ROC Curve for Ensemble (Full_Group)\nAccuracy: {accuracy:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(results_dir_name, f'roc_curve_Ensemble_Full_Group.png'))
plt.close()

train_dist = class_distribution(y_train)
test_dist = class_distribution(y_test)
results_records.append({
    "Model": "Ensemble",
    "Group": "Full_Group",
    "Train_Control_Count": train_dist.get(0, 0),
    "Train_Schizophrenia_Count": train_dist.get(1, 0),
    "Test_Control_Count": test_dist.get(0, 0),
    "Test_Schizophrenia_Count": test_dist.get(1, 0),
    "Accuracy": accuracy,
    "AUC": auc,
    "Control_Precision": report_dict["Control"]["precision"],
    "Control_Recall": report_dict["Control"]["recall"],
    "Control_F1": report_dict["Control"]["f1-score"],
    "Schizophrenia_Precision": report_dict["Schizophrenia"]["precision"],
    "Schizophrenia_Recall": report_dict["Schizophrenia"]["recall"],
    "Schizophrenia_F1": report_dict["Schizophrenia"]["f1-score"],
    "Macro_Avg_F1": report_dict["macro avg"]["f1-score"],
    "Time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
})


# ==========================================================
# Subgroup Evaluation (by Gender & Race)
# ==========================================================
group_combinations = [
    {'Gender': 'male',   'Race': 'white'},
    {'Gender': 'male',   'Race': 'black'},
    {'Gender': 'female', 'Race': 'white'},
    {'Gender': 'female', 'Race': 'black'},
]

for group in group_combinations:
    gender = group['Gender']
    race   = group['Race']
    subset = data[(data['Gender'] == gender) & (data['Race'] == race)]

    print(f"\nProcessing subgroup: {gender.capitalize()} / {race.capitalize()}")
    print(f"Total subgroup size: {len(subset)} samples")

    # Require minimum number of samples to train/test split
    if len(subset) > 5:
        X_subset = subset[features_to_keep]
        y_subset = subset['Group'].map(
            {'control': 0, 'schizophrenia': 1}
        ).astype('int8')

        # For each subgroup, we re-fit scaler + k-best on subgroup only
        X_subset_scaled = scaler.fit_transform(X_subset)
        X_subset_selected = kbest_selector.fit_transform(X_subset_scaled, y_subset)

        X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(
            X_subset_selected,
            y_subset,
            test_size=0.2,
            random_state=42,
            stratify=y_subset,
        )
        print(f"Training set size: {len(X_train_sg)} samples")
        print(f"Testing set size: {len(X_test_sg)} samples")
        group_name = f"{gender.capitalize()}_{race.capitalize()}"

        # Evaluate traditional ML models on subgroup
        evaluate_model(
            rf_model,
            'Random_Forest',
            X_train_sg,
            X_test_sg,
            y_train_sg,
            y_test_sg,
            group_name=group_name,
        )
        evaluate_model(
            svm_model,
            'SVM',
            X_train_sg,
            X_test_sg,
            y_train_sg,
            y_test_sg,
            group_name=group_name,
        )
        evaluate_model(
            mlp_model,
            'Neural_Network',
            X_train_sg,
            X_test_sg,
            y_train_sg,
            y_test_sg,
            group_name=group_name,
        )

        # Sequence reshapes for subgroup
        X_train_sg_lstm = X_train_sg.reshape(
            (X_train_sg.shape[0], 1, X_train_sg.shape[1])
        )
        X_test_sg_lstm = X_test_sg.reshape(
            (X_test_sg.shape[0], 1, X_test_sg.shape[1])
        )
        X_train_sg_cnn = X_train_sg.reshape(
            (X_train_sg.shape[0], X_train_sg.shape[1], 1)
        )
        X_test_sg_cnn = X_test_sg.reshape(
            (X_test_sg.shape[0], X_test_sg.shape[1], 1)
        )

        # Fresh Keras models per subgroup
        lstm_model_sg = create_lstm_model((1, X_train_sg.shape[1]))
        evaluate_model_keras(
            lstm_model_sg,
            'LSTM',
            X_train_sg_lstm,
            X_test_sg_lstm,
            y_train_sg,
            y_test_sg,
            group_name=group_name,
        )

        gru_model_sg = create_gru_model((1, X_train_sg.shape[1]))
        evaluate_model_keras(
            gru_model_sg,
            'GRU',
            X_train_sg_lstm,
            X_test_sg_lstm,
            y_train_sg,
            y_test_sg,
            group_name=group_name,
        )

        cnn_model_sg = create_cnn_model((X_train_sg.shape[1], 1))
        evaluate_model_keras(
            cnn_model_sg,
            'CNN',
            X_train_sg_cnn,
            X_test_sg_cnn,
            y_train_sg,
            y_test_sg,
            group_name=group_name,
        )
    else:
        print(f"Skipping {gender.capitalize()} / {race.capitalize()} due to insufficient samples (< 5).")


# ==========================================================
# Save Evaluation Summary
# ==========================================================
results_df = pd.DataFrame(results_records)
csv_file_path = os.path.join(results_dir_name, "results_summary.csv")
results_df.to_csv(csv_file_path, index=False)
print(f"\nResults have been saved to {csv_file_path}")
